question,answer
What is padding?,"Padding is the addition of extra layers of zeros around the input data, commonly used in convolutional neural networks (CNNs) to preserve spatial information during convolution."
Sigmoid Vs Softmax,"Sigmoid is used for binary classification, while Softmax is used for multi-class classification problems. Softmax converts raw scores into probabilities, ensuring they sum to 1 for multiple classes."
What is PoS Tagging?,"Part-of-Speech (PoS) tagging is the process of assigning a part of speech (e.g., noun, verb) to each word in a sentence. It helps in understanding the syntactic structure of a sentence."
What is tokenization?,"Tokenization is the process of breaking down text into individual units, such as words or subwords, for analysis."
What is topic modeling?,Topic modeling is a technique in natural language processing (NLP) to identify topics present in a text corpus. Common methods include Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF).
What is back propagation?,"Backpropagation is the training algorithm for neural networks, where the error is propagated backward, updating the weights to minimize the difference between predicted and actual outputs."
What is the idea behind GANs?,"Generative Adversarial Networks (GANs) consist of a generator and a discriminator trained simultaneously through adversarial training. The generator aims to produce realistic data, while the discriminator tries to distinguish real from fake."
What is the Computational Graph?,"A computational graph is a visual representation of mathematical operations in a neural network. Nodes represent operations, and edges represent the flow of data between them."
"What is sigmoid, and what does it do?","Sigmoid is an activation function that squashes input values between 0 and 1. It is often used in the output layer of a binary classification model, providing probabilities."
What is Named-Entity Recognition?,"Named-Entity Recognition (NER) is a task in NLP that involves identifying and classifying named entities (e.g., names of people, organizations) in text."
Explain the masked language model.,"A masked language model is a type of language model where certain words in a sentence are masked, and the model is trained to predict these masked words based on context."
How do you preprocess text in NLP?,"Text preprocessing in NLP includes tasks like tokenization, lowercasing, removing stop words, stemming, and lemmatization to prepare text data for analysis."
How do you extract features in NLP?,"Features in NLP can be extracted through methods like TF-IDF, word embeddings (Word2Vec, GloVe), and more advanced techniques like BERT embeddings."
How is wordvec different from Glove?,"Word2Vec and GloVe are both methods to create word embeddings. Word2Vec is based on neural networks, while GloVe is based on global word-word co-occurrence statistics."
What Are the Different Layers on CNN?,"Common layers in a CNN include convolutional layers, pooling layers, and fully connected layers."
What makes CNNs translation invariant?,"CNNs are translation invariant due to the use of shared weights in convolutional layers, allowing the model to recognize patterns regardless of their position in the input."
How is fastText different from wordvec?,"FastText is an extension of Word2Vec that considers subword information, making it suitable for handling out-of-vocabulary words and morphological variations."
Explain Generative Adversarial Network.,"GANs consist of a generator and a discriminator trained adversarially. The generator creates realistic data, while the discriminator aims to distinguish real from fake, leading to improved generative performance."
What is backward and forward propagation?,"Forward propagation is the process of passing input data through the network to generate predictions, while backward propagation is the process of updating weights by propagating the error backward during training."
What are Syntactic and Semantic Analysis?,"Syntactic analysis deals with the structure of sentences, while semantic analysis focuses on the meaning of words and sentences."
What is a local optimum?,A local optimum is a point in the parameter space where the function has a lower value than its neighbors but is not necessarily the global minimum.
Explain gates used in LSTM with their functions.,"LSTM (Long Short-Term Memory) networks use gates such as input gate, forget gate, and output gate to regulate the flow of information through the cell state, enabling the model to capture long-term dependencies."
"What is ReLU, and how is it better than sigmoid or tanh?",ReLU (Rectified Linear Unit) is an activation function that introduces non-linearity and helps with the vanishing gradient problem. It is computationally efficient and often performs better than sigmoid or tanh.
"What is transfer learning, and have you used it before?","Transfer learning involves using pre-trained models on a different task to boost performance on a new, related task. Yes/No depending on personal experience."
"What is multi-task learning, and when should it be used?","Multi-task learning involves training a model to perform multiple tasks simultaneously. It should be used when tasks have related features, and learning them together can improve overall performance."
Difference between convex and non-convex cost function.,"Convex cost functions have a single global minimum, while non-convex functions may have multiple minima. Training neural networks involves dealing with non-convex optimization problems."
Why do we remove stop words? When do we not remove them?,"Stop words (common words like ""the,"" ""and"") are removed to focus on meaningful words. They may not be removed when maintaining context, as in some information retrieval tasks."
"Explain the difference between an epoch, a batch, and an iteration.","An epoch is a complete pass through the entire training dataset. A batch is a subset of the dataset processed together, and an iteration is the number of batches processed in one epoch."
What is the difference between NLP and NLU?,"NLP (Natural Language Processing) deals with the interaction between computers and human language, while NLU (Natural Language Understanding) focuses on extracting meaning from language."
"For online learning, which one would you prefer: SGD or Adagrad, and why?","SGD (Stochastic Gradient Descent) is often preferred for online learning due to its faster updates and adaptability to changing data distributions. Adagrad may accumulate historical gradients, leading to less adaptability."
What Is a Multi-layer Perceptron (MLP)?,"MLP is a type of neural network with multiple layers, including input, hidden, and output layers."
Is it always bad to have local optima?,Local optima can hinder finding the global optimum but may still yield acceptable results. Escaping local optima depends on the optimization algorithm used.
"In node2vec, what does embedding represent: topological similarity or nearness?","Node embeddings in node2vec represent both topological similarity and nearness, capturing structural information in the graph."
What do you understand by Boltzmann Machine and Restricted Boltzmann Machines?,Boltzmann Machines are probabilistic graphical models. Restricted Boltzmann Machines (RBMs) are a simplified version used for learning probability distributions over data.
How to compute an inverse matrix faster by playing around with some computational tricks?,Techniques like LU decomposition or using specialized algorithms can speed up matrix inversion.
"For infrequent/rare words, which among CBOW and SkipGram should be used for wordvec training?","SkipGram is better for infrequent words as it focuses on predicting context words given a target word, allowing the model to capture rare word associations."
What is pooling in CNN? Why do we need it?,"Pooling in CNN involves down-sampling the spatial dimensions of the input, reducing computational complexity and capturing the most essential information."
Describe the structure of Artificial Neural Networks & RNN (recurrent neural network).,"Artificial Neural Networks consist of input, hidden, and output layers. RNNs have connections that form cycles, allowing them to capture sequential information."
How to Select a Batch Size? Will selecting a batch size produce better or worse results?,"Batch size affects training efficiency. A smaller batch size may offer more updates but with higher variance, while a larger batch size may provide more stable updates at the cost of computational efficiency."
What are N-grams? How can we use them?,"N-grams are contiguous sequences of n items from a given sample of text or speech. They are used in text analysis, language modeling, and machine translation to capture local dependencies."
How large should be N for our bag of words when using N-grams?,"The choice of N in N-grams depends on the specific application and the desired level of context. Common values include 1 (unigrams), 2 (bigrams), and 3 (trigrams)."
How can you use neural nets for text classification and computer vision?,"In text classification, neural networks process word embeddings or sequences of words. In computer vision, convolutional neural networks (CNNs) process image data for tasks like object recognition."
Do gradient descent methods always converge at the same point?,"No, the convergence point can vary due to factors like the optimization algorithm, learning rate, and the problem's nature."
What is gradient descent? How does it work?,Gradient descent is an optimization algorithm that iteratively adjusts model parameters to minimize the cost function by moving in the direction of steepest descent (negative gradient).
What are autoencoders? Explain the different layers of autoencoders and mention three practical usages of them.,"Autoencoders are neural networks designed to encode input data into a lower-dimensional representation. Layers include an encoder, bottleneck layer, and decoder. Usages include data compression, denoising, and feature learning."
What is vanishing gradient descent?,"Vanishing gradient descent occurs when gradients become extremely small during backpropagation, hindering the learning process, especially in deep networks."
Difference between Vanishing gradient Vs Exploding gradient?,"Vanishing gradient involves gradients becoming very small, while exploding gradient involves gradients becoming very large, both affecting the training stability of neural networks."
How to handle dying node problems in case of ReLU activation function?,"Leaky ReLU or Parametric ReLU can be used to address dying ReLU problems by allowing a small gradient for negative inputs, preventing neurons from becoming inactive."
What is the use of the leaky ReLU function?,"Leaky ReLU allows a small, non-zero gradient for negative inputs, preventing neurons from becoming inactive and mitigating the dying ReLU problem."
What are the different Deep Learning Frameworks?,"Deep learning frameworks include TensorFlow, PyTorch, Keras, Theano, and MXNet, among others."
What is the difference between machine learning and deep learning?,Machine learning is a broader field that includes various algorithms for learning patterns from data. Deep learning is a subset of machine learning that specifically focuses on neural networks with multiple layers.
"What is a dropout layer, and how does it help a neural network?",Dropout is a regularization technique that randomly drops (sets to zero) a fraction of neurons during training to prevent overfitting and improve generalization.
Explain why dropout in a neural network acts as a regularizer.,"Dropout acts as a regularizer by preventing neurons from relying too much on specific features, encouraging the network to learn more robust and generalizable representations."
How to know whether your model is suffering from the problem of Exploding Gradients?,"Exploding gradients can be observed by monitoring the loss during training. If the loss becomes extremely large, it indicates a potential issue with exploding gradients."
How to handle the exploding gradient problem?,"Techniques to handle exploding gradients include gradient clipping, normalization techniques (like Batch Normalization), or using different optimization algorithms."
How Does an LSTM Network Work?,"LSTM networks have memory cells and gates that control the flow of information, enabling them to capture and remember long-term dependencies in sequential data."
What problem does Bi-LSTM solve instead of only LSTM?,"Bi-LSTM (Bidirectional LSTM) processes input sequences in both forward and backward directions, capturing contextual information from both past and future, addressing limitations in unidirectional LSTMs."
What is the difference between LSTM and GRU?,LSTM and GRU (Gated Recurrent Unit) are both types of recurrent neural networks. The key difference is in their architecture and the number of gates they use to control information flow.
What happens to the predictions of a CNN if an image is rotated?,A well-trained CNN should still make accurate predictions for rotated images due to its ability to capture translation-invariant features through convolutional layers.
How does CNN help in translation and rotation invariance of images?,"CNNs use shared weights in convolutional layers, enabling them to capture features that are translation-invariant and rotation-invariant, making them robust to these transformations."
Define Term Frequency & Inverse Document Frequency (Tf-idf) and how to use it for converting text to vector.,"Term Frequency (TF) measures the frequency of a term in a document, while Inverse Document Frequency (IDF) measures how unique a term is across documents. TF-IDF is the product of TF and IDF and is used to convert text to a vector."
What are three primary convolutional neural network layers? How are they commonly put together?,"The three primary CNN layers are convolutional layers, pooling layers, and fully connected layers. They are commonly stacked in the order of convolutional, activation, pooling, and fully connected layers."
Describe the architecture of a typical Convolutional Neural Network.,"A typical CNN consists of alternating convolutional and pooling layers, followed by fully connected layers. The convolutional layers capture local features, and the fully connected layers make predictions based on these"
"What do you mean by Dropout and Batch Normalization, When and why use",Dropout is a regularization technique where randomly selected neurons are ignored during training to prevent overfitting. Batch Normalization normalizes input batches to improve training speed and stability. They are used to enhance generalization and prevent overfitting.
What is the difference between online and batch learning,"Online learning updates the model with each new training example, while batch learning updates the model only after processing the entire training dataset. Online learning is faster but can be less accurate. Batch learning provides accurate updates but is computationally expensive."
Is dropout used on the test set,"No, dropout is typically not used during the test or inference phase. It is only applied during training to prevent overfitting."
What is an activation function and discuss the use of an activation function,"An activation function introduces non-linearity to the neural network, enabling it to learn complex patterns. It determines the output of a neuron given its input. Without activation functions, the neural network would behave like a linear model, unable to capture intricate patterns."
Explain three different types of activation functions,"Three common activation functions are Sigmoid (outputs between 0 and 1), Tanh (outputs between -1 and 1), and ReLU (outputs max(0, input))."
What is the range of activation functions,"Sigmoid ranges from 0 to 1, Tanh from -1 to 1, and ReLU from 0 to positive infinity."
Why is Rectified Linear Unit a good activation function,ReLU is computationally efficient and helps mitigate the vanishing gradient problem. It allows the network to learn faster and is widely used in hidden layers.
Why don't we use the ReLU activation function in the output layer,ReLU isn't used in the output layer for tasks requiring probability distributions or when negative values are meaningful. It's often replaced with softmax for multi-class classification.
What can go wrong if we use a linear activation instead of ReLU,"A linear activation in hidden layers would make the neural network behave like a linear model, limiting its ability to capture complex patterns and reducing its expressive power."
Give examples in which a many-to-one RNN architecture is appropriate,"Sentiment analysis, where multiple words form a sentiment, and only the final sentiment is relevant, is an example of a many-to-one RNN architecture."
What is RNN and How does an RNN work,"Recurrent Neural Network (RNN) is a type of neural network designed for sequence data. It has loops to allow information persistence, enabling it to remember previous inputs in the sequence."
Why Sigmoid or Tanh is not preferred to be used as the activation function in the hidden layer of the neural network,"Sigmoid and Tanh suffer from the vanishing gradient problem, making them less effective in training deep networks. ReLU and its variants are often preferred in hidden layers."
"Difference between various activation functions such as Sigmoid, Tanh, Softmax, ReLU, Leaky ReLU","Sigmoid squashes values between 0 and 1, Tanh between -1 and 1, Softmax converts raw scores to probabilities, ReLU outputs the input for positive values and 0 otherwise, and Leaky ReLU allows a small gradient for negative inputs."
Why Tanh activation function preferred over sigmoid,"Tanh is preferred over Sigmoid in hidden layers because it has a wider range (-1 to 1), helping mitigate the vanishing gradient problem better than Sigmoid."
What are word embeddings Why are they useful,Word embeddings are dense vector representations of words in a continuous vector space. They capture semantic relationships between words and are useful in NLP tasks by providing a more meaningful representation of words.
What is WordVec,"WordVec, short for Word Vectors, refers to techniques like Word2Vec that learn word embeddings by capturing relationships between words in a continuous vector space."
What are some advantages of using character embeddings instead of word embeddings,"Character embeddings allow the model to handle out-of-vocabulary words and capture morphological information, useful in scenarios with misspellings or rare words."
"How do you get sentence meanings from word embeddings, considering the position of words in the sentence","By combining word embeddings and considering the order of words, methods like averaging or using recurrent neural networks (RNNs) can be used to obtain sentence embeddings."
Would you prefer gradient boosting trees model or logistic regression when doing text classification with bag of words,Logistic regression is often preferred for simplicity and efficiency in text classification tasks with bag-of-words representations. Gradient boosting trees may be computationally expensive.
What is bag of words How we can use it for text vectorization,"Bag of Words represents text as an unordered set of words and their frequencies. It can be used for text vectorization by creating a matrix where each row corresponds to a document, and columns correspond to unique words with their frequencies."
What are the advantages and disadvantages of bag of words,"Advantages include simplicity and efficiency. Disadvantages include the loss of word order and semantics, treating each word independently."
What is the main difference between Adam and SGD,Adam (Adaptive Moment Estimation) is an optimization algorithm that combines the advantages of both Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). It adapts learning rates for each parameter individually. SGD (Stochastic Gradient Descent) uses a constant learning rate for all parameters.
What are the advantages and disadvantages of SGD over gradient descent,"Advantages of SGD include faster convergence, while disadvantages include noisy updates and oscillations in the loss function."
"What is the difference between stochastic gradient descent SGD and gradient descent GD, Batch gradient descent, Stochastic gradient descent, Mini-batch gradient descent, what are the pros and cons for each of them","GD uses the entire dataset for each update, Stochastic GD uses a single random sample, and Mini-batch GD uses a subset. Pros of GD include less noise, cons include high computational cost. Pros of SGD include faster convergence, cons include noisy updates. Mini-batch combines advantages of both."
When would you use GD over SGD and vice-versa,Use GD when the dataset is small and can fit into memory. Use SGD for large datasets where the memory footprint is a concern.
How would you choose the number of filters and the filter size at each CNN layer,"The number of filters determines the depth of the layer, capturing different features. Filter size depends on the expected size of features. Empirical testing and domain knowledge guide these choices."
How can we use CNN for text classification,"By treating text as an image (using techniques like word embeddings or one-hot encoding), CNNs can be applied to capture local patterns and dependencies for text classification tasks."
What are some advantages in using a CNN (convolutional neural network rather than a DNN (dense neural network in an image classification task,"CNNs are advantageous due to their ability to capture local patterns, translational invariance, and parameter sharing. They are well-suited for image-related tasks."
Describe two ways to visualize features of a CNN in an image classification task,Visualizing filters or feature maps can provide insights into what patterns the CNN has learned. Grad-CAM (Gradient-weighted Class Activation Mapping) highlights important regions in the input image.
Why do segmentation CNNs typically have an encoder-decoder style / structure,Encoder-decoder structures in segmentation CNNs allow the network to capture features at different scales and produce high-resolution outputs by combining low-level and high-level features.
What is a convolutional layer & Why do we actually need convolutions Can we use fully-connected layers for that,"Convolutional layers apply filters to input data, capturing local patterns. Convolution is computationally efficient and reduces the number of parameters. Fully-connected layers are less suitable for images due to the huge number of connections."
What are the advantages of parameter sharing in case of convolution,"Parameter sharing in convolution allows the same filter to be applied across different parts of the input, capturing the same feature in different locations. This reduces the number of parameters, making the model more efficient."
Why do we use convolutions for images rather than just Fully Connected layers,"Convolutions exploit local spatial correlations, reducing the number of parameters and enabling the model to learn hierarchical features. Fully connected layers would lead to an impractical number of connections."
Why would you use many small convolutional kernels such as x rather than a few large ones,"Using many small kernels captures complex features and increases non-linearity, enabling the network to learn more expressive patterns. It's computationally more efficient than using a few large kernels."
Why we generally use Softmax non-linearity function as the last operation in-network,"Softmax converts raw scores into probabilities, making it suitable for multi-class classification problems, where the final layer represents class probabilities."
How does Batch Normalization differ in training and inferencing,"During training, Batch Normalization normalizes activations per batch. During inference, a running mean and variance are used based on the statistics accumulated during training."
How does batch size affect training of neural networks,Larger batch sizes generally lead to faster convergence but require more memory. Smaller batch sizes introduce noise but can adapt faster to changes in the data.
"When using mini batch gradient descent, why is it important to shuffle the data","Shuffling the data ensures that each mini-batch is representative of the overall dataset, preventing the model from learning spurious correlations present in the order of the data."
Give a simple mathematical argument why a mini-batch version of such ML algorithm might be computationally more efficient than a training with the full dataset,"Mini-batch training utilizes parallel processing capabilities and reduces memory requirements, making it computationally more efficient compared to processing the full dataset in one go."
On a simplified and fundamental scale what makes the newly developed BERT model better than traditional NLP models,"BERT (Bidirectional Encoder Representations from Transformers) captures context bidirectionally, considering both left and right context words simultaneously. Traditional models typically process text in a unidirectional manner, missing contextual information."
How would you initialize weights in a neural network,"Weights are often initialized randomly using techniques like Glorot initialization or He initialization, which help prevent vanishing or exploding gradients during training."
Why weights are initialized with small random numbers in a neural network What happens when weights are all or constant values,"Small random weights prevent neurons from always learning the same features, promoting diversity in the network. If weights are all constant, neurons in a layer would learn the same feature, limiting the model's expressiveness."
Suppose you have a NN with layers and ReLU activations What will happen if we initialize all the weights with the same value,"Initializing all weights with the same value may cause neurons to learn the same features, preventing the network from effectively capturing diverse patterns. This is known as symmetry breaking."
What is backpropagation How does it work Why do we need it,Backpropagation is a supervised learning algorithm used to train artificial neural networks. It involves computing the gradient of the loss function with respect to the weights through the chain rule of calculus. It adjusts weights to minimize the error between predicted and actual outputs.
Why large filter sizes in early layers can be a bad choice How to choose filter size,"Large filter sizes in early layers may capture overly broad features, losing fine-grained details. Choosing filter size depends on the task and expected feature sizes. Smaller filters are often preferred for detailed pattern recognition."
Which one is more powerful a layer decision tree or a -layer neural network without any activation function (Hint: non-linearity),A deep neural network is generally more powerful than a decision tree without any activation function. The ability to learn non-linear relationships makes neural networks more expressive for complex tasks.
Both decision trees and deep neural networks are non-linear classifiers; they separate the space by a complicated decision boundary. Why then is it so much easier for us to intuitively follow a decision tree model vs a deep neural network,"Decision trees provide a transparent and interpretable structure, making it easier for humans to understand the decision-making process. Neural networks, especially deep ones, have complex architectures that are harder to interpret intuitively."
"If you could take advantage of multiple CPU cores, would you prefer a boosted-tree algorithm over a random forest","Boosted-tree algorithms like XGBoost or LightGBM are often more computationally intensive and can benefit from parallel processing. However, the choice between boosted trees and random forests depends on various factors, including dataset size, characteristics, and computational resources."
What are MSE and RMSE,"Mean Squared Error (MSE) measures the average squared difference between predicted and actual values. Root Mean Squared Error (RMSE) is the square root of MSE, providing a measure in the original units of the data. Both assess the goodness of fit of a regression model."
Explain DBSCAN algorithm,"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that groups data points based on their density. It defines clusters as dense regions separated by sparser areas, discovering clusters of arbitrary shapes and identifying outliers as noise."
What are dummy variables,"Dummy variables, also known as indicator variables, are binary variables used to represent categorical data in statistical models. They encode categorical variables with two or more categories into numerical format, allowing them to be used in regression and other models."
What is anomaly detection,"Anomaly detection is the identification of patterns or instances that deviate significantly from the norm in a dataset. It is used to find outliers, novelties, or irregularities that may indicate errors, fraud, or other unusual behavior."
What is Bayesian inference,Bayesian inference is a statistical method for updating probabilities based on new evidence. It uses Bayes' theorem to estimate the probability of a hypothesis given observed data. It involves combining prior beliefs with likelihood to obtain a posterior probability.
What is the R-Squared value,"The R-Squared value (coefficient of determination) measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s) in a regression model. It ranges from 0 to 1, with higher values indicating a better fit."
What about ordinal features,"Ordinal features are categorical variables with a clear ordering or ranking of categories, but the intervals between them are not well-defined. They can be treated like categorical variables or assigned numerical values based on their order."
Loss functions in regression,"Common loss functions in regression include Mean Squared Error (MSE), Mean Absolute Error (MAE), and Huber loss. These measure the difference between predicted and actual values, guiding the model during training."
What are Recommender Systems,"Recommender Systems are algorithms that provide personalized suggestions or recommendations to users based on their preferences, behavior, or similarity to other users. They are widely used in e-commerce, streaming services, and other applications."
Undersampling vs Oversampling,"Undersampling reduces the size of the majority class, and oversampling increases the size of the minority class in imbalanced datasets. Both techniques aim to balance class distribution for better model performance."
What is reinforcement learning,"Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment. It receives feedback in the form of rewards or penalties, allowing the agent to optimize its behavior over time."
Do we call Knn a lazy algorithm,"Yes, K-Nearest Neighbors (KNN) is considered a lazy or instance-based algorithm because it doesn't build a model during training. Instead, it memorizes the training data and classifies new instances based on their proximity to existing instances."
Define a Monte Carlo simulation,Monte Carlo simulation is a computational technique that uses random sampling to model and analyze complex systems. It involves running multiple simulations with random input values to estimate the distribution of possible outcomes and assess risk or uncertainty.
Does Kmeans and Kmeans++ is the same,"No, K-Means and K-Means++ are not the same. K-Means++ is an improvement over K-Means in terms of initialization. K-Means++ selects initial cluster centers more intelligently, leading to better convergence and more accurate results."
What is pruning in Decision Tree,Pruning in decision trees involves removing branches that contribute little to the overall accuracy or generalization of the tree. It helps prevent overfitting and simplifies the model without sacrificing performance.
How does an XGB control overfitting,"XGBoost controls overfitting by incorporating regularization terms in its objective function, using techniques such as shrinkage (learning rate), maximum depth constraints, and subsampling of data. These measures prevent the model from fitting noise in the training data."
What is the class imbalance problem,"The class imbalance problem occurs when the distribution of classes in a dataset is not equal, with one class significantly outnumbering the others. It can lead to biased models that perform poorly on the minority class."
Why is lightGBM prone to overfitting,"LightGBM, a gradient boosting framework, is prone to overfitting when it's allowed to create deep trees or is trained on small datasets. It's important to tune hyperparameters and use regularization techniques to mitigate overfitting."
Name any one distance-based algorithm,K-Nearest Neighbors (KNN) is a distance-based algorithm that classifies data points based on the majority class of their k-nearest neighbors in the feature space.
What is the objective function for Knn,KNN doesn't have a traditional objective function like other algorithms. Its objective is to classify a new data point based on the majority class of its k-nearest neighbors.
What is the standard error of the mean,The standard error of the mean (SEM) measures the variability of sample means from different samples drawn from the same population. It is the standard deviation of sample means and reflects the precision of the sample mean as an estimate of the population mean.
What are some disadvantages of K-means,"K-Means has limitations such as sensitivity to initial cluster centers, the need to specify the number of clusters (K), susceptibility to outliers, and difficulty handling non-spherical or unevenly sized clusters."
What is data augmentation Give examples,"Data augmentation involves creating new training examples by applying various transformations to the existing data. Examples include image rotation, flipping, zooming, and adjusting brightness. It helps improve model generalization."
What is Euclidean and Manhattan distance,Euclidean distance is the straight-line distance between two points in a Euclidean space. Manhattan distance (L1 norm) is the sum of the absolute differences between corresponding coordinates of two points in a grid or space.
What is the role of gamma in RBF kernels,"In support vector machines (SVM) with radial basis function (RBF) kernels, gamma determines the shape of the decision boundary. A higher gamma makes the decision boundary more flexible, fitting the training data more closely, potentially leading to overfitting."
How would you handle an imbalanced dataset,"Handling an imbalanced dataset can involve techniques like resampling (undersampling or oversampling), using different evaluation metrics (precision, recall), generating synthetic data, or using specialized algorithms designed for imbalanced data."
Is it a good idea to combine multiple trees,"Combining multiple trees, as in ensemble methods like Random Forest or Gradient Boosting, is a good idea as it helps improve model performance, reduce overfitting, and provide more robust predictions."
How do support vector machine algorithms work,"Support Vector Machines (SVM) aim to find the hyperplane that maximally separates different classes in the feature space. The algorithm identifies support vectors, which are data points closest to the decision boundary, to define the optimal hyperplane."
What is the significance of Residual Networks,"Residual Networks (ResNets) address the vanishing gradient problem in deep neural networks by introducing skip connections or shortcuts. These allow the direct flow of information through the network, enabling the training of very deep architectures."
What does it mean to have low MAE and high MSE,"Low Mean Absolute Error (MAE) indicates that, on average, predictions are close to the true values. High Mean Squared Error (MSE) indicates greater variability or dispersion in prediction errors, with larger errors having more influence."
What are the disadvantages of linear regression,"Linear regression assumes a linear relationship, sensitive to outliers, and may not capture complex non-linear patterns. It also assumes homoscedasticity (constant variance of errors) and independence of errors, which may not always hold."
What is a recommendation engine How does it work,"A recommendation engine suggests items to users based on their preferences and behavior. It can use collaborative filtering, content-based filtering, or hybrid methods to analyze user data and make personalized recommendations."
What is K-means How can you select K for K-means,"K-Means is a clustering algorithm that partitions data into K clusters based on similarity. Selecting the optimal K can involve methods like the elbow method, silhouette score, or cross-validation to find the number of clusters that best represents the data."
Does Radial basis kernel function is there in SVM,"Yes, the radial basis function (RBF) kernel is commonly used in SVM. It is a popular choice for non-linear classification tasks, introducing a non-linear decision boundary in the feature space."
What is linear regression Why is it called linear,"Linear regression is a statistical method that models the relationship between a dependent variable and one or more independent variables. It is called ""linear"" because the relationship is assumed to be a linear combination of the input variables."
Is pruning always a good method to construct a tree,"Pruning is a good method to prevent overfitting and simplify a decision tree. However, in some cases, pruning may remove branches that capture important patterns, leading to an oversimplified model that lacks predictive power."
What is the difference between bagging and boosting,"Bagging (Bootstrap Aggregating) builds multiple models independently and combines their predictions, reducing variance. Boosting builds models sequentially, emphasizing the training instances with higher errors, improving accuracy and reducing bias."
Which algorithm uses margin to classify the classes,"Support Vector Machines (SVM) use a margin, which is the distance between the decision boundary and the nearest data point of any class. SVM aims to maximize this margin to improve generalization to unseen data."
What algorithm can be used to summarize Twitter feed,"Text summarization algorithms, such as extractive or abstractive summarization methods, can be used to summarize Twitter feeds. These algorithms analyze the content of tweets and generate concise summaries."
How do you generate arbitrary or random shape clusters,"Hierarchical clustering algorithms, DBSCAN, or density-based methods can be used to generate arbitrary or random shape clusters by considering the density and connectivity of data points rather than assuming spherical clusters."
How to compute standard error of the median in a simple way,"Standard error of the median can be computed using the formula: SE(median)?1.253×IQR?SE(median)?1.253×N?IQR?, where IQR is the interquartile range and N is the sample size."
How does GBDTs decide to split a node What does it minimize,"Gradient Boosted Decision Trees (GBDTs) decide to split a node by minimizing the loss function, which measures the difference between predicted and actual values. It iteratively fits new trees to the residuals, gradually improving the overall model."
What is the difference between R-squared and Adjusted R-squared,"R-squared measures the proportion of the variance in the dependent variable explained by the independent variables. Adjusted R-squared adjusts for the number of predictors, penalizing for the inclusion of irrelevant variables, providing a more reliable goodness-of-fit measure."
How is matrix factorization useful in recommendation systems,"Matrix factorization is useful in recommendation systems by decomposing a user-item interaction matrix into latent factor matrices for users and items. This helps discover hidden patterns and preferences, making personalized recommendations."
What are the approximation methods in Reinforcement Learning,"Approximation methods in Reinforcement Learning include function approximation techniques, such as linear function approximation or deep neural networks, to represent value functions or policies in high-dimensional state spaces."
What is the difference between an error and a residual error,"In statistics, an error refers to the difference between the true and predicted values, while a residual error specifically denotes the difference between the observed and predicted values in the context of regression analysis."
Why does training an SVM take a long time How can I speed up,"Training an SVM can take a long time due to the quadratic time complexity in the number of samples. To speed up training, one can use optimization techniques, kernel approximation methods, or reduce the size of the dataset."
"Difference between bagging, boosting, and the relation to Bayes' theorem","Bagging and boosting are ensemble learning methods that combine multiple models. Bagging reduces variance, while boosting reduces bias. The relation to Bayes' theorem lies in their iterative nature, updating the model based on errors, similar to Bayesian updating."
Which algorithm takes the data to the next dimension and then classifies,"Kernel methods, such as the Support Vector Machine (SVM) with non-linear kernels (e.g., RBF kernel), implicitly map data into a higher-dimensional space to make it more separable before classifying."
What are categorical variables and what do we do with categorical variables,"Categorical variables represent qualitative data with discrete categories. In data analysis, they are often encoded as dummy variables for use in models, ensuring compatibility with algorithms that require numerical input."
How would you handle an imbalanced dataset,"Handling an imbalanced dataset can involve techniques like resampling (undersampling or oversampling), using different evaluation metrics (precision, recall), generating synthetic data, or using specialized algorithms designed for imbalanced data."
What is RNN and How does an RNN work,"Recurrent Neural Networks (RNNs) are a type of neural network designed for sequence data. They have connections that form cycles, allowing them to maintain a memory of previous inputs. This enables them to capture temporal dependencies in data."
Does Radial basis kernel function is there in SVM,"Yes, the radial basis function (RBF) kernel is commonly used in SVM. It is a popular choice for non-linear classification tasks, introducing a non-linear decision boundary in the feature space."
What is the role of gamma in RBF kernels,"In support vector machines (SVM) with radial basis function (RBF) kernels, gamma determines the shape of the decision boundary. A higher gamma makes the decision boundary more flexible, fitting the training data more closely, potentially leading to overfitting."
Why Sigmoid or Tanh is not preferred to be used as the activation function in the hidden layer of the neural network,"Sigmoid and tanh activation functions are prone to vanishing gradient problems, especially in deep networks. This hinders the learning process, and alternatives like ReLU or variants are often preferred for hidden layers."
How does BatchNormalization differ in training and inferencing,"During training, Batch Normalization normalizes activations per batch. During inference, a running mean and variance are used based on the statistics accumulated during training."
What are some advantages of using character embeddings instead of word embeddings,"Character embeddings capture sub-word information and are beneficial for handling misspellings, morphological variations, and rare words. They can also be more compact and generalize better to unseen words."
Why don't we use the Relu activation function in the output layer,"ReLU activation in the output layer is not suitable for classification tasks with multiple classes because it doesn't provide normalized probabilities. Softmax activation is preferred for multi-class classification, ensuring that output values sum to 1."
How does GBDTs decide to split a node What does it minimize,"Gradient Boosted Decision Trees (GBDTs) decide to split a node by minimizing the loss function, which measures the difference between predicted and actual values. It iteratively fits new trees to the residuals, gradually improving the overall model."
Why large filter sizes in early layers can be a bad choice How to choose filter size,"Large filter sizes in early layers may capture overly broad features, losing fine-grained details. Choosing filter size depends on the task and expected feature sizes. Smaller filters are often preferred for detailed pattern recognition."
